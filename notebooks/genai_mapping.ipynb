{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afffcfa",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9152c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309b34dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9b184",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e357100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… System Loaded: Happy Hacking.\n"
     ]
    }
   ],
   "source": [
    "## Import Block \n",
    "## SYSTEM SETUP & HELPER FUNCTIONS (You are free to edit as needed)\n",
    "# --- ğŸ› ï¸ SYSTEM SETUP & HELPER FUNCTIONS (Do not edit unless necessary) ---\n",
    "\n",
    "# 1. Imports\n",
    "import duckdb\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Environment\n",
    "try:\n",
    "    from IPython.display import display, HTML\n",
    "    IN_NOTEBOOK = True\n",
    "except ImportError:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "# 2. Database Wrapper Class\n",
    "class DuckDBWrapper:\n",
    "    def __init__(self, duckdb_path=None):\n",
    "        self.db_path = Path(duckdb_path).resolve() if duckdb_path else None\n",
    "        self.con = None\n",
    "        self.registered_tables = [] \n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establishes connection and loads HTTPFS for remote files.\"\"\"\n",
    "        if self.con: return\n",
    "        try:\n",
    "            if self.db_path:\n",
    "                self.con = duckdb.connect(str(self.db_path), read_only=False)\n",
    "            else:\n",
    "                self.con = duckdb.connect(database=':memory:', read_only=False)\n",
    "            self.con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Connection Failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def close(self):\n",
    "        if self.con:\n",
    "            try: self.con.close()\n",
    "            except: pass\n",
    "            self.con = None\n",
    "\n",
    "    def register_data_view(self, paths, table_names):\n",
    "        \"\"\"Creates virtual views for Parquet/CSV/JSON files (Zero-Copy).\"\"\"\n",
    "        if not self.con: self.connect()\n",
    "        if len(paths) != len(table_names): raise ValueError(\"Length mismatch\")\n",
    "        \n",
    "        for path, table_name in zip(paths, table_names):\n",
    "            path_str = str(path)\n",
    "            if not glob.glob(path_str) and not os.path.exists(path_str): continue\n",
    "            try:\n",
    "                # Logic: Detect filetype and use appropriate DuckDB reader\n",
    "                if \".parquet\" in path_str: query = f\"CREATE OR REPLACE VIEW {table_name} AS SELECT * FROM read_parquet('{path_str}')\"\n",
    "                elif \".csv\" in path_str: query = f\"CREATE OR REPLACE VIEW {table_name} AS SELECT * FROM read_csv_auto('{path_str}')\"\n",
    "                elif \".json\" in path_str: query = f\"CREATE OR REPLACE VIEW {table_name} AS SELECT * FROM read_json_auto('{path_str}')\"\n",
    "                else: continue\n",
    "                \n",
    "                self.con.execute(query)\n",
    "                if table_name not in self.registered_tables: self.registered_tables.append(table_name)\n",
    "            except Exception as e: print(f\"âŒ Error registering {table_name}: {e}\")\n",
    "\n",
    "    def run_query(self, sql_query, show_results=False):\n",
    "        \"\"\"Executes SQL. Returns DataFrame. Displays scrollable HTML if show_results=True.\"\"\"\n",
    "        if not self.con: self.connect()\n",
    "        import polars as pl \n",
    "        \n",
    "        try:\n",
    "            arrow_table = self.con.execute(sql_query).arrow()\n",
    "            df = pl.DataFrame(arrow_table)\n",
    "            \n",
    "            if show_results:\n",
    "                if IN_NOTEBOOK:\n",
    "                    # ğŸ’¡ UI FEATURE: Pandas for reliable HTML Table rendering\n",
    "                    pdf = df.head(1000).to_pandas()\n",
    "                    table_html = pdf.to_html(index=False, border=0, classes=[\"dataframe\"])\n",
    "                    scrollable_div = f\"\"\"\n",
    "                    <div style=\"max-height: 400px; overflow-y: auto; overflow-x: auto; border: 1px solid #444;\">\n",
    "                        <style>.dataframe thead th {{ position: sticky; top: 0; background: #222; color: white; }}</style>\n",
    "                        {table_html}\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                    display(HTML(scrollable_div))\n",
    "                else:\n",
    "                    self._print_simple_table(df)\n",
    "                return None\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Query Failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def show_tables(self):\n",
    "        \"\"\"Show tables using the Brighter Rich style.\"\"\"\n",
    "        df = self.run_query(\"SELECT table_name, table_type FROM information_schema.tables WHERE table_schema='main'\", show_results=False)\n",
    "        if df is not None:\n",
    "            self._print_fancy_table(df, title=\"ğŸ“‚ Database Assets\")\n",
    "\n",
    "    def show_schema(self, table_name):\n",
    "        \"\"\"Show schema using the Brighter Rich style.\"\"\"\n",
    "        query = f\"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{table_name}'\"\n",
    "        df = self.run_query(query, show_results=False)\n",
    "        if df is not None:\n",
    "            self._print_fancy_table(df, title=f\"ğŸ“‹ Schema: {table_name}\")\n",
    "\n",
    "    def _print_fancy_table(self, df, title):\n",
    "        \"\"\"A High-Contrast, Bright table for metadata.\"\"\"\n",
    "        from rich.console import Console; from rich.table import Table; from rich import box\n",
    "        console = Console()\n",
    "        table = Table(title=title, title_style=\"bold bright_yellow\", header_style=\"bold bright_white\", box=box.ROUNDED, show_lines=True, border_style=\"bright_black\")\n",
    "        for col in df.columns: table.add_column(col, style=\"bright_cyan\", justify=\"left\")\n",
    "        for row in df.iter_rows(named=True): table.add_row(*[str(v) for v in row.values()])\n",
    "        console.print(table)\n",
    "\n",
    "    def _print_simple_table(self, df):\n",
    "        \"\"\"Fallback table for terminal.\"\"\"\n",
    "        from rich.console import Console; from rich.table import Table; from rich import box\n",
    "        console = Console()\n",
    "        table = Table(title=\"Query Results\", box=box.SIMPLE, show_lines=False)\n",
    "        for col in df.columns: table.add_column(col, style=\"dim\", no_wrap=True, overflow=\"ellipsis\", max_width=30)\n",
    "        for row in df.head(10).iter_rows(named=True): table.add_row(*[str(v) if v is not None else \"\" for v in row.values()])\n",
    "        console.print(table)\n",
    "\n",
    "    def export(self, data, file_name, file_type=\"csv\", output_dir=\"../data/exports\"):\n",
    "        \"\"\"Smart Export.\"\"\"\n",
    "        if isinstance(data, str):\n",
    "            print(f\"â³ Running query for export: '{file_name}'...\")\n",
    "            df = self.run_query(data, show_results=False)\n",
    "        else:\n",
    "            df = data\n",
    "        if df is None or df.height == 0: print(\"âš ï¸ Export skipped (Empty/None)\"); return\n",
    "\n",
    "        full_path = Path(output_dir) / f\"{file_name}.{file_type}\"\n",
    "        full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        try:\n",
    "            if file_type == \"parquet\": df.write_parquet(str(full_path))\n",
    "            elif file_type == \"csv\": df.write_csv(str(full_path))\n",
    "            elif file_type == \"json\": df.write_ndjson(str(full_path))\n",
    "            else: print(f\"âŒ Unknown format: {file_type}\"); return\n",
    "            print(f\"âœ… Exported {df.height} rows to: {full_path}\")\n",
    "        except Exception as e: print(f\"âŒ Write failed: {e}\")\n",
    "\n",
    "# 3. Project Helper Functions\n",
    "def setup_database_environment(db_path, fresh_start=False):\n",
    "    \"\"\"\n",
    "    Initializes DuckDB. Uses RENAME strategy for reliable Fresh Start.\n",
    "    \"\"\"\n",
    "    db_path = Path(db_path).resolve()\n",
    "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Handle Fresh Start via RENAME (Avoids Lock Issues)\n",
    "    if fresh_start and db_path.exists():\n",
    "        print(f\"ğŸ§¹ Fresh Start: Resetting {db_path.name}...\")\n",
    "        gc.collect() # Garbage collect old connections\n",
    "        \n",
    "        trash_path = db_path.with_suffix(\".duckdb.old\")\n",
    "        if trash_path.exists():\n",
    "            try: trash_path.unlink() \n",
    "            except: pass\n",
    "            \n",
    "        try:\n",
    "            shutil.move(str(db_path), str(trash_path))\n",
    "            if db_path.with_suffix(\".duckdb.wal\").exists(): db_path.with_suffix(\".duckdb.wal\").unlink()\n",
    "            print(\"   âœ“ Old database moved to trash (Connection Reset).\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Warning: Could not move old DB: {e}. Attempting direct overwrite.\")\n",
    "\n",
    "    # 2. Connect\n",
    "    con = DuckDBWrapper(duckdb_path=db_path)\n",
    "    con.connect()\n",
    "    print(f\"ğŸ”Œ Connected to: {db_path}\")\n",
    "    return con\n",
    "\n",
    "def download_and_cache_data(file_list, base_url, data_dir):\n",
    "    data_dir = Path(data_dir); data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    paths, names = [], []\n",
    "    print(\"\\nğŸš€ Checking Data Assets...\")\n",
    "    for filename in file_list:\n",
    "        local_path = data_dir / filename\n",
    "        url = f\"{base_url}/{filename}\"; table_name = Path(filename).stem\n",
    "        if local_path.exists() and local_path.stat().st_size > 0:\n",
    "            print(f\"ğŸ“‚ Cached: '{table_name}'\"); paths.append(local_path); names.append(table_name); continue\n",
    "        print(f\"â¬‡ï¸  Downloading '{filename}'...\")\n",
    "        for attempt in range(1, 4):\n",
    "            try:\n",
    "                with requests.get(url, stream=True, headers={'Connection': 'close'}, timeout=(10, 60)) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(local_path, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192): f.write(chunk)\n",
    "                print(f\"âœ… Saved to {local_path}\"); paths.append(local_path); names.append(table_name); time.sleep(1); break\n",
    "            except Exception as e:\n",
    "                if local_path.exists(): local_path.unlink()\n",
    "                if attempt < 3: time.sleep(2)\n",
    "                else: print(f\"âŒ Failed {filename}: {e}\")\n",
    "    return paths, names\n",
    "\n",
    "def process_local_files(file_list):\n",
    "    \"\"\"\n",
    "    Scans for local files and prepares them for registration.\n",
    "    Args:\n",
    "        file_list: List of file paths (strings).\n",
    "    Returns:\n",
    "        paths (list[Path]), names (list[str])\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    names = []\n",
    "    print(\"\\nğŸ” Scanning Local Files...\")\n",
    "    for f in file_list:\n",
    "        p = Path(f).resolve()\n",
    "        if p.exists() and p.is_file():\n",
    "            print(f\"âœ… Found: {p.name}\")\n",
    "            paths.append(p)\n",
    "            names.append(p.stem) # 'data.csv' -> 'data'\n",
    "        else:\n",
    "            print(f\"âŒ Not Found: {f}\")\n",
    "    return paths, names\n",
    "\n",
    "def run_project_sql_pipeline(con, sql_folder, export_folder, output_format=\"parquet\"):\n",
    "    \"\"\"Runs all .sql files, exports, and registers. Defaults to Parquet.\"\"\"\n",
    "    sql_dir = Path(sql_folder); export_dir = Path(export_folder)\n",
    "    print(f\"\\nğŸš€ Starting SQL Pipeline (Output: {output_format})...\")\n",
    "    for sql_path in sorted(sql_dir.glob(\"*.sql\")):\n",
    "        print(f\"â–¶ï¸ Running {sql_path.name}...\")\n",
    "        try: con.export(sql_path.read_text(), sql_path.stem, output_format, export_dir)\n",
    "        except Exception as e: print(f\"âŒ Failed {sql_path.name}: {e}\")\n",
    "    extension = f\".{output_format}\"\n",
    "    result_paths = sorted(export_dir.glob(f\"*{extension}\"))\n",
    "    if result_paths:\n",
    "        con.register_data_view(result_paths, [p.stem for p in result_paths])\n",
    "        print(f\"âœ“ Pipeline finished. {len(result_paths)} processed tables registered.\")\n",
    "    else: print(\"âš ï¸ Pipeline finished but no output files were found.\")\n",
    "\n",
    "print(\"âœ… System Loaded: Happy Hacking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f92d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Fresh Start: Resetting test.duckdb...\n",
      "   âœ“ Old database moved to trash (Connection Reset).\n",
      "ğŸ”Œ Connected to: /home/erikbrown01/stop-super-speeders-hackathon/data/data/duckdb/test.duckdb\n",
      "\n",
      "ğŸš€ Checking Data Assets...\n",
      "ğŸ“‚ Cached: 'nyc_speed_cameras_historic'\n",
      "ğŸ“‚ Cached: 'test1_nyc_speed_cameras'\n",
      "ğŸ“‚ Cached: 'test2_nyc_speed_cameras'\n",
      "ğŸ“‚ Cached: 'test3_nyc_speed_cameras'\n",
      "ğŸ“‚ Cached: 'nyc_traffic_violations_historic'\n",
      "ğŸ“‚ Cached: 'test1_nyc_traffic_violations'\n",
      "ğŸ“‚ Cached: 'test2_nyc_traffic_violations'\n",
      "ğŸ“‚ Cached: 'test3_nyc_traffic_violations'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">               ğŸ“‚ Database Assets               </span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #ffffff; text-decoration-color: #ffffff; font-weight: bold\"> table_name                      </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #ffffff; text-decoration-color: #ffffff; font-weight: bold\"> table_type </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> nyc_speed_cameras_historic      </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> nyc_traffic_violations_historic </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> test1_nyc_speed_cameras         </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> test1_nyc_traffic_violations    </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> test2_nyc_speed_cameras         </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> test2_nyc_traffic_violations    </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> test3_nyc_speed_cameras         </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> test3_nyc_traffic_violations    </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\"> VIEW       </span><span style=\"color: #808080; text-decoration-color: #808080\">â”‚</span>\n",
       "<span style=\"color: #808080; text-decoration-color: #808080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;93m               ğŸ“‚ Database Assets               \u001b[0m\n",
       "\u001b[90mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[1;97m \u001b[0m\u001b[1;97mtable_name                     \u001b[0m\u001b[1;97m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[1;97m \u001b[0m\u001b[1;97mtable_type\u001b[0m\u001b[1;97m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mnyc_speed_cameras_historic     \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mnyc_traffic_violations_historic\u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mtest1_nyc_speed_cameras        \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mtest1_nyc_traffic_violations   \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mtest2_nyc_speed_cameras        \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mtest2_nyc_traffic_violations   \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mtest3_nyc_speed_cameras        \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\u001b[0m\n",
       "\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mtest3_nyc_traffic_violations   \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\u001b[96m \u001b[0m\u001b[96mVIEW      \u001b[0m\u001b[96m \u001b[0m\u001b[90mâ”‚\u001b[0m\n",
       "\u001b[90mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set Up your DuckDB Warehouse and Download Datasets\n",
    "\n",
    "FRESH_START = True  # Set to True to wipe DB and start clean\n",
    "DB_PATH     = \"../data/duckdb/test.duckdb\"\n",
    "DATA_DIR    = \"../data/opendata\"\n",
    "BASE_URL    = \"https://fastopendata.org/dssg-safestreets\"\n",
    "\n",
    "# Define the files you want to download/load\n",
    "TARGET_FILES = [\n",
    "    \"nyc_speed_cameras_historic.parquet\",\n",
    "    \"test1_nyc_speed_cameras.json\",\n",
    "    \"test2_nyc_speed_cameras.csv\",\n",
    "    \"test3_nyc_speed_cameras.csv\",\n",
    "    \"nyc_traffic_violations_historic.parquet\",\n",
    "    \"test1_nyc_traffic_violations.json\",\n",
    "    \"test2_nyc_traffic_violations.csv\",\n",
    "    \"test3_nyc_traffic_violations.csv\"\n",
    "]\n",
    "\n",
    "# --- ğŸš€ EXECUTION ---\n",
    "# 1. Initialize Database\n",
    "con = setup_database_environment(DB_PATH, fresh_start=FRESH_START)\n",
    "\n",
    "# 2. Download Data & Register Views\n",
    "file_paths, table_names = download_and_cache_data(TARGET_FILES, BASE_URL, DATA_DIR)\n",
    "con.register_data_view(file_paths, table_names)\n",
    "\n",
    "# 3. Show what we have\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f436f78",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with SQK, show_results=True enables scrollable table\n",
    "query = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "SELECT * FROM nyc_speed_cameras_historic limit 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "historic_df = con.run_query(query, show_results=False).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0efb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with SQK, show_results=True enables scrollable table\n",
    "query = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "SELECT * FROM test2_nyc_traffic_violations limit 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "upload_df = con.run_query(query, show_results=False).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a049ff03",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dafaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d334c68",
   "metadata": {},
   "source": [
    "# Date Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d08553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Pydantic Schema for the Transformation Rule ---\n",
    "class DateParsingRule(BaseModel):\n",
    "    columns_used: List[str] = Field(\n",
    "        ..., \n",
    "        description=\"The exact list of column names needed to construct the date.\"\n",
    "    )\n",
    "    join_separator: str = Field(\n",
    "        \" \", \n",
    "        description=\"The separator to use if joining multiple columns (default to space ' '). If single column, this is ignored.\"\n",
    "    )\n",
    "    source_strptime_format: str = Field(\n",
    "        ..., \n",
    "        description=\"The Python strptime format string matching the source data (e.g., '%Y-%m-%d' or '%B %d, %Y'). If multiple columns, this format must match the joined string.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673945e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rows = upload_df.head(10).to_markdown()\n",
    "columns_available = list(upload_df.columns)\n",
    "target_format_description = historic_df[\"issue_date\"][0]  # Assume we want to match the first date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d53cb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Rows from Upload Data:\n",
      " |    | License Id   | County      |   Age | Violation Code                    |   Violation Year |   Violation Month |   Points |   Birth Year |   Birth Month |\n",
      "|---:|:-------------|:------------|------:|:----------------------------------|-----------------:|------------------:|---------:|-------------:|--------------:|\n",
      "|  0 | BR425170     | Westchester |    70 | SPEED IN ZONE 21-30               |             2025 |                11 |        6 |         1954 |            12 |\n",
      "|  1 | PK119730     | Other       |    30 | CHILD SAFETY RESTRAINT            |             2025 |                11 |        3 |         1994 |            12 |\n",
      "|  2 | JW166375     | Nassau      |    19 | SPEED IN ZONE 31-40               |             2025 |                11 |        8 |         2005 |            12 |\n",
      "|  3 | UK266096     | Richmond    |    43 | FAILED TO STOP SCHOOL BUS         |             2025 |                11 |        5 |         1981 |            12 |\n",
      "|  4 | JO742886     | Queens      |    19 | USE OF PORTABLE ELECTRONIC DEVICE |             2025 |                11 |        5 |         2005 |            12 |\n",
      "|  5 | NA519050     | Westchester |    39 | UNREGISTERED VEHICLE              |             2025 |                11 |        0 |         1985 |            12 |\n",
      "|  6 | FF608247     | Suffolk     |    19 | FAIL TO SIGNAL                    |             2025 |                11 |        2 |         2005 |            12 |\n",
      "|  7 | HR029404     | Other       |    23 | DRIVING LEFT OF CENTER            |             2025 |                11 |        3 |         2001 |            12 |\n",
      "|  8 | LR222562     | New York    |    28 | SPEED IN ZONE 1-10                |             2025 |                11 |        3 |         1996 |            12 |\n",
      "|  9 | IG102555     | Queens      |    41 | DISOBEY TRAFFIC DEVICE            |             2025 |                11 |        2 |         1983 |            12 |\n",
      "\n",
      "Available Columns: ['License Id', 'County', 'Age', 'Violation Code', 'Violation Year', 'Violation Month', 'Points', 'Birth Year', 'Birth Month']\n",
      "\n",
      "Target Date Format Example: 2025-04-13 08:00:00-04:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample Rows from Upload Data:\\n\", example_rows)\n",
    "print(\"\\nAvailable Columns:\", columns_available)\n",
    "print(\"\\nTarget Date Format Example:\", target_format_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ea6ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Logic Identified:\n",
      "Columns: ['Violation Year', 'Violation Month']\n",
      "Format: '%Yâ€“%m'\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Generate the Transformation Rule ---\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=f\"You are a Python Data Engineer. I have a dataset with the following columns: {columns_available}\\nHere are the first few rows: {example_rows}\\nI need to transform the data into this format: {target_format_description}.\\nIdentify the columns that contain date information and define a parsing rule.\\nIf the date is split across multiple columns, specify the order they should be joined and the format string to parse the result.\",\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": DateParsingRule.model_json_schema(),\n",
    "    },\n",
    ")\n",
    "\n",
    "rule = DateParsingRule.model_validate_json(response.text)\n",
    "\n",
    "print(f\"LLM Logic Identified:\")\n",
    "print(f\"Columns: {rule.columns_used}\")\n",
    "print(f\"Format: '{rule.source_strptime_format}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "708be57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transformed Data ---\n",
      "Original: {'License Id': 'BR425170', 'County': 'Westchester', 'Age': 70, 'Violation Code': 'SPEED IN ZONE 21-30', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 6, 'Birth Year': 1954, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'PK119730', 'County': 'Other', 'Age': 30, 'Violation Code': 'CHILD SAFETY RESTRAINT', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 3, 'Birth Year': 1994, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'JW166375', 'County': 'Nassau', 'Age': 19, 'Violation Code': 'SPEED IN ZONE 31-40', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 8, 'Birth Year': 2005, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'UK266096', 'County': 'Richmond', 'Age': 43, 'Violation Code': 'FAILED TO STOP SCHOOL BUS', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 5, 'Birth Year': 1981, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'JO742886', 'County': 'Queens', 'Age': 19, 'Violation Code': 'USE OF PORTABLE ELECTRONIC DEVICE', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 5, 'Birth Year': 2005, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'NA519050', 'County': 'Westchester', 'Age': 39, 'Violation Code': 'UNREGISTERED VEHICLE', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 0, 'Birth Year': 1985, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'FF608247', 'County': 'Suffolk', 'Age': 19, 'Violation Code': 'FAIL TO SIGNAL', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 2, 'Birth Year': 2005, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'HR029404', 'County': 'Other', 'Age': 23, 'Violation Code': 'DRIVING LEFT OF CENTER', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 3, 'Birth Year': 2001, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'LR222562', 'County': 'New York', 'Age': 28, 'Violation Code': 'SPEED IN ZONE 1-10', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 3, 'Birth Year': 1996, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n",
      "Original: {'License Id': 'IG102555', 'County': 'Queens', 'Age': 41, 'Violation Code': 'DISOBEY TRAFFIC DEVICE', 'Violation Year': 2025, 'Violation Month': 11, 'Points': 2, 'Birth Year': 1983, 'Birth Month': 12} -> Transformed Date: 2025-11-01\n"
     ]
    }
   ],
   "source": [
    "# --- 4. APPLY the Transformation (The \"ETL\" part) ---\n",
    "# This function uses the LLM's output to actually process the data\n",
    "def transform_row(row, rule: DateParsingRule):\n",
    "    try:\n",
    "        # 1. Extract values based on the columns the LLM identified\n",
    "        values = [str(row[col]) for col in rule.columns_used]\n",
    "        \n",
    "        # 2. Join them (e.g. \"2023 October 25\")\n",
    "        raw_date_string = rule.join_separator.join(values)\n",
    "        \n",
    "        # 3. Parse using the LLM's format string\n",
    "        dt_object = datetime.strptime(raw_date_string, rule.source_strptime_format)\n",
    "        \n",
    "        # 4. Return in the target format (ISO 8601)\n",
    "        return dt_object.strftime(\"%Y-%m-%d\")\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Test on the data\n",
    "print(\"\\n--- Transformed Data ---\")\n",
    "for row in upload_df.head(10).to_dict(orient=\"records\"):\n",
    "    new_date = transform_row(row, rule)\n",
    "    print(f\"Original: {row} -> Transformed Date: {new_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d74e",
   "metadata": {},
   "source": [
    "# Column Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Pydantic Schema ---\n",
    "class ColumnPair(BaseModel):\n",
    "    historical_column: str = Field(\n",
    "        description=\"The exact column name from the historical data.\"\n",
    "    )\n",
    "    new_upload_column: Optional[str] = Field(\n",
    "        description=\"The matching column name from the new upload. None if no match found.\"\n",
    "    )\n",
    "\n",
    "class MappingResult(BaseModel):\n",
    "    mappings: List[ColumnPair] = Field(\n",
    "        description=\"A list of mappings covering every column in the historical dataset.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35aab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "    You are a data engineering assistant. You are tasked with mapping the output from a new file to historical data. \n",
    "    Below are the first 10 rows of the historical data, and the first 10 rows of the new uploaded data. \n",
    "    Based on these rows print a dictionary where the key is the column name from the historical data, \n",
    "    and the values are the column names from new upload. Each column from the historical data should appear in the dictionary. \n",
    "    If a column from the historical data does not have a match then its value should be None. \n",
    "    Every column from the new uploaded data does not need a to appear in the final dictionary. \n",
    "    If a column from the new upload does not have a match in the historical data, it should not appear in the final dictionary. \n",
    "    Only print the dictionary as the final output. \n",
    "\"\"\"\n",
    "\n",
    "requirments = \"\"\"\"\n",
    "    Requirements:\n",
    "    1. Return a list of mappings.\n",
    "    2. Every column from the 'Historical Columns' list MUST appear in the output.\n",
    "    3. If a historical column has no match in the new file, the value must be None.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# --- 3. Generate Content ---\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=f\"{prompt}\\n Example of historical columns:\\n {historic_df.head(10).to_string()}\\n Example of upload columns:\\n {upload_df.head(10).to_string()}\\n {requirments}\",\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": MappingResult.model_json_schema(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ceb36a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"mappings\": [{\"historical_column\": \"issue_date\", \"new_upload_column\": \"issue_date\"}, {\"historical_column\": \"created_at\", \"new_upload_column\": \"created_at\"}, {\"historical_column\": \"amount_due\", \"new_upload_column\": \"amount_due\"}, {\"historical_column\": \"county\", \"new_upload_column\": \"county\"}, {\"historical_column\": \"fine_amount\", \"new_upload_column\": \"fine_amount\"}, {\"historical_column\": \"interest_amount\", \"new_upload_column\": \"interest_amount\"}, {\"historical_column\": \"issuing_agency\", \"new_upload_column\": \"issuing_agency\"}, {\"historical_column\": \"judgment_entry_date\", \"new_upload_column\": \"judgment_entry_date\"}, {\"historical_column\": \"license_type\", \"new_upload_column\": \"license_type\"}, {\"historical_column\": \"payment_amount\", \"new_upload_column\": \"payment_amount\"}, {\"historical_column\": \"penalty_amount\", \"new_upload_column\": \"penalty_amount\"}, {\"historical_column\": \"plate\", \"new_upload_column\": \"plate\"}, {\"historical_column\": \"precinct\", \"new_upload_column\": \"precinct\"}, {\"historical_column\": \"reduction_amount\", \"new_upload_column\": \"reduction_amount\"}, {\"historical_column\": \"state\", \"new_upload_column\": \"state\"}, {\"historical_column\": \"summons_number\", \"new_upload_column\": \"summons_number\"}, {\"historical_column\": \"violation\", \"new_upload_column\": \"violation\"}, {\"historical_column\": \"violation_status\", \"new_upload_column\": \"violation_status\"}, {\"historical_column\": \"violation_time\", \"new_upload_column\": \"violation_time\"}]}'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stop-super-speeders-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
