{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Block \n",
    "## SYSTEM SETUP & HELPER FUNCTIONS (You are free to edit as needed)\n",
    "# --- üõ†Ô∏è SYSTEM SETUP & HELPER FUNCTIONS (Do not edit unless necessary) ---\n",
    "\n",
    "# 1. Imports\n",
    "import duckdb\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Environment\n",
    "try:\n",
    "    from IPython.display import display, HTML\n",
    "    IN_NOTEBOOK = True\n",
    "except ImportError:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "# 2. Database Wrapper Class\n",
    "class DuckDBWrapper:\n",
    "    def __init__(self, duckdb_path=None):\n",
    "        self.db_path = Path(duckdb_path).resolve() if duckdb_path else None\n",
    "        self.con = None\n",
    "        self.registered_tables = [] \n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establishes connection and loads HTTPFS for remote files.\"\"\"\n",
    "        if self.con: return\n",
    "        try:\n",
    "            if self.db_path:\n",
    "                self.con = duckdb.connect(str(self.db_path), read_only=False)\n",
    "            else:\n",
    "                self.con = duckdb.connect(database=':memory:', read_only=False)\n",
    "            self.con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Connection Failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def close(self):\n",
    "        if self.con:\n",
    "            try: self.con.close()\n",
    "            except: pass\n",
    "            self.con = None\n",
    "\n",
    "    def register_data_view(self, paths, table_names):\n",
    "        \"\"\"Creates virtual views for Parquet/CSV/JSON files (Zero-Copy).\"\"\"\n",
    "        if not self.con: self.connect()\n",
    "        if len(paths) != len(table_names): raise ValueError(\"Length mismatch\")\n",
    "        \n",
    "        for path, table_name in zip(paths, table_names):\n",
    "            path_str = str(path)\n",
    "            if not glob.glob(path_str) and not os.path.exists(path_str): continue\n",
    "            try:\n",
    "                # Logic: Detect filetype and use appropriate DuckDB reader\n",
    "                if \".parquet\" in path_str: query = f\"CREATE OR REPLACE VIEW {table_name} AS SELECT * FROM read_parquet('{path_str}')\"\n",
    "                elif \".csv\" in path_str: query = f\"CREATE OR REPLACE VIEW {table_name} AS SELECT * FROM read_csv_auto('{path_str}')\"\n",
    "                elif \".json\" in path_str: query = f\"CREATE OR REPLACE VIEW {table_name} AS SELECT * FROM read_json_auto('{path_str}')\"\n",
    "                else: continue\n",
    "                \n",
    "                self.con.execute(query)\n",
    "                if table_name not in self.registered_tables: self.registered_tables.append(table_name)\n",
    "            except Exception as e: print(f\"‚ùå Error registering {table_name}: {e}\")\n",
    "\n",
    "    def run_query(self, sql_query, show_results=False):\n",
    "        \"\"\"Executes SQL. Returns DataFrame. Displays scrollable HTML if show_results=True.\"\"\"\n",
    "        if not self.con: self.connect()\n",
    "        import polars as pl \n",
    "        \n",
    "        try:\n",
    "            arrow_table = self.con.execute(sql_query).arrow()\n",
    "            df = pl.DataFrame(arrow_table)\n",
    "            \n",
    "            if show_results:\n",
    "                if IN_NOTEBOOK:\n",
    "                    # üí° UI FEATURE: Pandas for reliable HTML Table rendering\n",
    "                    pdf = df.head(1000).to_pandas()\n",
    "                    table_html = pdf.to_html(index=False, border=0, classes=[\"dataframe\"])\n",
    "                    scrollable_div = f\"\"\"\n",
    "                    <div style=\"max-height: 400px; overflow-y: auto; overflow-x: auto; border: 1px solid #444;\">\n",
    "                        <style>.dataframe thead th {{ position: sticky; top: 0; background: #222; color: white; }}</style>\n",
    "                        {table_html}\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                    display(HTML(scrollable_div))\n",
    "                else:\n",
    "                    self._print_simple_table(df)\n",
    "                return None\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query Failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def show_tables(self):\n",
    "        \"\"\"Show tables using the Brighter Rich style.\"\"\"\n",
    "        df = self.run_query(\"SELECT table_name, table_type FROM information_schema.tables WHERE table_schema='main'\", show_results=False)\n",
    "        if df is not None:\n",
    "            self._print_fancy_table(df, title=\"üìÇ Database Assets\")\n",
    "\n",
    "    def show_schema(self, table_name):\n",
    "        \"\"\"Show schema using the Brighter Rich style.\"\"\"\n",
    "        query = f\"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{table_name}'\"\n",
    "        df = self.run_query(query, show_results=False)\n",
    "        if df is not None:\n",
    "            self._print_fancy_table(df, title=f\"üìã Schema: {table_name}\")\n",
    "\n",
    "    def _print_fancy_table(self, df, title):\n",
    "        \"\"\"A High-Contrast, Bright table for metadata.\"\"\"\n",
    "        from rich.console import Console; from rich.table import Table; from rich import box\n",
    "        console = Console()\n",
    "        table = Table(title=title, title_style=\"bold bright_yellow\", header_style=\"bold bright_white\", box=box.ROUNDED, show_lines=True, border_style=\"bright_black\")\n",
    "        for col in df.columns: table.add_column(col, style=\"bright_cyan\", justify=\"left\")\n",
    "        for row in df.iter_rows(named=True): table.add_row(*[str(v) for v in row.values()])\n",
    "        console.print(table)\n",
    "\n",
    "    def _print_simple_table(self, df):\n",
    "        \"\"\"Fallback table for terminal.\"\"\"\n",
    "        from rich.console import Console; from rich.table import Table; from rich import box\n",
    "        console = Console()\n",
    "        table = Table(title=\"Query Results\", box=box.SIMPLE, show_lines=False)\n",
    "        for col in df.columns: table.add_column(col, style=\"dim\", no_wrap=True, overflow=\"ellipsis\", max_width=30)\n",
    "        for row in df.head(10).iter_rows(named=True): table.add_row(*[str(v) if v is not None else \"\" for v in row.values()])\n",
    "        console.print(table)\n",
    "\n",
    "    def export(self, data, file_name, file_type=\"csv\", output_dir=\"../data/exports\"):\n",
    "        \"\"\"Smart Export.\"\"\"\n",
    "        if isinstance(data, str):\n",
    "            print(f\"‚è≥ Running query for export: '{file_name}'...\")\n",
    "            df = self.run_query(data, show_results=False)\n",
    "        else:\n",
    "            df = data\n",
    "        if df is None or df.height == 0: print(\"‚ö†Ô∏è Export skipped (Empty/None)\"); return\n",
    "\n",
    "        full_path = Path(output_dir) / f\"{file_name}.{file_type}\"\n",
    "        full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        try:\n",
    "            if file_type == \"parquet\": df.write_parquet(str(full_path))\n",
    "            elif file_type == \"csv\": df.write_csv(str(full_path))\n",
    "            elif file_type == \"json\": df.write_ndjson(str(full_path))\n",
    "            else: print(f\"‚ùå Unknown format: {file_type}\"); return\n",
    "            print(f\"‚úÖ Exported {df.height} rows to: {full_path}\")\n",
    "        except Exception as e: print(f\"‚ùå Write failed: {e}\")\n",
    "\n",
    "# 3. Project Helper Functions\n",
    "def setup_database_environment(db_path, fresh_start=False):\n",
    "    \"\"\"\n",
    "    Initializes DuckDB. Uses RENAME strategy for reliable Fresh Start.\n",
    "    \"\"\"\n",
    "    db_path = Path(db_path).resolve()\n",
    "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Handle Fresh Start via RENAME (Avoids Lock Issues)\n",
    "    if fresh_start and db_path.exists():\n",
    "        print(f\"üßπ Fresh Start: Resetting {db_path.name}...\")\n",
    "        gc.collect() # Garbage collect old connections\n",
    "        \n",
    "        trash_path = db_path.with_suffix(\".duckdb.old\")\n",
    "        if trash_path.exists():\n",
    "            try: trash_path.unlink() \n",
    "            except: pass\n",
    "            \n",
    "        try:\n",
    "            shutil.move(str(db_path), str(trash_path))\n",
    "            if db_path.with_suffix(\".duckdb.wal\").exists(): db_path.with_suffix(\".duckdb.wal\").unlink()\n",
    "            print(\"   ‚úì Old database moved to trash (Connection Reset).\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Warning: Could not move old DB: {e}. Attempting direct overwrite.\")\n",
    "\n",
    "    # 2. Connect\n",
    "    con = DuckDBWrapper(duckdb_path=db_path)\n",
    "    con.connect()\n",
    "    print(f\"üîå Connected to: {db_path}\")\n",
    "    return con\n",
    "\n",
    "def download_and_cache_data(file_list, base_url, data_dir):\n",
    "    data_dir = Path(data_dir); data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    paths, names = [], []\n",
    "    print(\"\\nüöÄ Checking Data Assets...\")\n",
    "    for filename in file_list:\n",
    "        local_path = data_dir / filename\n",
    "        url = f\"{base_url}/{filename}\"; table_name = Path(filename).stem\n",
    "        if local_path.exists() and local_path.stat().st_size > 0:\n",
    "            print(f\"üìÇ Cached: '{table_name}'\"); paths.append(local_path); names.append(table_name); continue\n",
    "        print(f\"‚¨áÔ∏è  Downloading '{filename}'...\")\n",
    "        for attempt in range(1, 4):\n",
    "            try:\n",
    "                with requests.get(url, stream=True, headers={'Connection': 'close'}, timeout=(10, 60)) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(local_path, 'wb') as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192): f.write(chunk)\n",
    "                print(f\"‚úÖ Saved to {local_path}\"); paths.append(local_path); names.append(table_name); time.sleep(1); break\n",
    "            except Exception as e:\n",
    "                if local_path.exists(): local_path.unlink()\n",
    "                if attempt < 3: time.sleep(2)\n",
    "                else: print(f\"‚ùå Failed {filename}: {e}\")\n",
    "    return paths, names\n",
    "\n",
    "def process_local_files(file_list):\n",
    "    \"\"\"\n",
    "    Scans for local files and prepares them for registration.\n",
    "    Args:\n",
    "        file_list: List of file paths (strings).\n",
    "    Returns:\n",
    "        paths (list[Path]), names (list[str])\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    names = []\n",
    "    print(\"\\nüîç Scanning Local Files...\")\n",
    "    for f in file_list:\n",
    "        p = Path(f).resolve()\n",
    "        if p.exists() and p.is_file():\n",
    "            print(f\"‚úÖ Found: {p.name}\")\n",
    "            paths.append(p)\n",
    "            names.append(p.stem) # 'data.csv' -> 'data'\n",
    "        else:\n",
    "            print(f\"‚ùå Not Found: {f}\")\n",
    "    return paths, names\n",
    "\n",
    "def run_project_sql_pipeline(con, sql_folder, export_folder, output_format=\"parquet\"):\n",
    "    \"\"\"Runs all .sql files, exports, and registers. Defaults to Parquet.\"\"\"\n",
    "    sql_dir = Path(sql_folder); export_dir = Path(export_folder)\n",
    "    print(f\"\\nüöÄ Starting SQL Pipeline (Output: {output_format})...\")\n",
    "    for sql_path in sorted(sql_dir.glob(\"*.sql\")):\n",
    "        print(f\"‚ñ∂Ô∏é Running {sql_path.name}...\")\n",
    "        try: con.export(sql_path.read_text(), sql_path.stem, output_format, export_dir)\n",
    "        except Exception as e: print(f\"‚ùå Failed {sql_path.name}: {e}\")\n",
    "    extension = f\".{output_format}\"\n",
    "    result_paths = sorted(export_dir.glob(f\"*{extension}\"))\n",
    "    if result_paths:\n",
    "        con.register_data_view(result_paths, [p.stem for p in result_paths])\n",
    "        print(f\"‚úì Pipeline finished. {len(result_paths)} processed tables registered.\")\n",
    "    else: print(\"‚ö†Ô∏è Pipeline finished but no output files were found.\")\n",
    "\n",
    "print(\"‚úÖ System Loaded: Happy Hacking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up your ingest\n",
    "\n",
    "FRESH_START = True  # Set to True to wipe DB and start clean\n",
    "DB_PATH     = \"../data/duckdb/test.duckdb\"\n",
    "DATA_DIR    = \"../data/opendata\"\n",
    "BASE_URL    = \"https://fastopendata.org/dssg-safestreets\"\n",
    "\n",
    "# Define the files you want to download/load\n",
    "TARGET_FILES = [\n",
    "    \"nyc_speed_cameras_historic.parquet\",\n",
    "    \"test1_nyc_speed_cameras.json\",\n",
    "    \"test2_nyc_speed_cameras.csv\",\n",
    "    \"test3_nyc_speed_cameras.csv\",\n",
    "    \"nyc_traffic_violations_historic.parquet\",\n",
    "    \"test1_nyc_traffic_violations.json\",\n",
    "    \"test2_nyc_traffic_violations.csv\",\n",
    "    \"test3_nyc_traffic_violations.csv\"\n",
    "]\n",
    "\n",
    "# --- üöÄ EXECUTION ---\n",
    "# 1. Initialize Database\n",
    "con = setup_database_environment(DB_PATH, fresh_start=FRESH_START)\n",
    "\n",
    "# 2. Download Data & Register Views\n",
    "file_paths, table_names = download_and_cache_data(TARGET_FILES, BASE_URL, DATA_DIR)\n",
    "con.register_data_view(file_paths, table_names)\n",
    "\n",
    "# 3. Show what we have\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Local Files\n",
    "MY_LOCAL_FILES = [\n",
    "    \"../seeds/point_values.csv\",\n",
    "\n",
    "]\n",
    "\n",
    "# 2. Initialize Database\n",
    "con = setup_database_environment(DB_PATH)\n",
    "\n",
    "# 3. Process & Register\n",
    "file_paths, table_names = process_local_files(MY_LOCAL_FILES)\n",
    "con.register_data_view(file_paths, table_names)\n",
    "\n",
    "# 4. Verify results\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Example: Check total count\n",
    "query = \"\"\"\n",
    "SELECT count(*) as total_rows \n",
    "FROM nyc_speed_cameras_historic\n",
    "\"\"\"\n",
    "con.run_query(query, show_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with SQK, show_results=True enables scrollable table\n",
    "query = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "SELECT * FROM nyc_speed_cameras_historic limit 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "con.run_query(query, show_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with SQK, show_results=True enables scrollable table\n",
    "query = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "SELECT * FROM nyc_traffic_violations_historic  limit 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "con.run_query(query, show_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Tables with SQLc(without show_results=True)\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "SELECT * from nyc_speed_cameras_historic limit 20000\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show specific table schema\n",
    "con.show_schema(\"nyc_speed_cameras_historic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the registered tables\n",
    "con.show_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Tables as CSV, JSON, or Parquet\n",
    "\n",
    "# 1. Define the SQL (or use an existing DataFrame variable)\n",
    "export_sql = \"\"\"\n",
    "SELECT * \n",
    "FROM test1_nyc_traffic_violations  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 2. Run & Save (One line!)\n",
    "con.export(\n",
    "    data=export_sql, \n",
    "    file_name=\"my_traffic_subset\", \n",
    "    file_type=\"csv\"\n",
    ")\n",
    "\n",
    "# Optional: You can specify a different folder if needed\n",
    "# con.export(export_sql, \"my_subset\", \"parquet\", output_dir=\"my_custom_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a SQL pipeline of .sql files in sql folder\n",
    "repo_root = Path.cwd().resolve().parent\n",
    "sql_folder = Path.cwd() / \"sql\"\n",
    "export_folder = repo_root / \"data\" / \"exports\"\n",
    "run_project_sql_pipeline(con, sql_folder, export_folder, output_format=\"csv\")\n",
    "\n",
    "# Verify that the new tables (e.g., ticket_summary) are now available\n",
    "con.show_tables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stop-super-speeders-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
